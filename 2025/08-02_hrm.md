# Hierarchical Reasoning Model

* Authors: Sapient Intelligence, Singapore
* Link: https://arxiv.org/abs/2506.21734
* Date: 2025-07-22

## Notes
* Transformers are fixed depth, not Turing complete, and not suitable for complex reasoning tasks.
* Chain of Thought (CoT): brittle task decomposition, extensive data requirements, and high latency.
* Hierarchical Reasoning Model (HRM): two interdependent recurrent modules: a high-level module responsible for slow,
  abstract planning, and a low-level module handling rapid, detailed computations.
* Latent reasoning: the model conducts computation within its internal hidden state space.
* Fast and slow updating to avoid rapid convergence to local minima.
* One-step gradient approximation to avoid backpropagation through time (BPTT): using the gradient of the last state of
  each module and treating other states as constant. Gradient path:
```
Output head -> final state of the H-module -> final state of the L-module -> input embedding
```
* Deep supervision: with multiple forward passes of the HRM model, detach the hidden state before backprop.
* Adaptive computational time (ACT): using Q-learning to adaptively determine the number of forward passes.
* Inference-time scaling: increasing maximum number of forward passes during inference improves Sudoku performance.
* Post-norm RMSNorm and AdamW optimizer for stable Q-learning training.
* 27 million parameters, 1000 training examples, no pretraining: ARC-AGI 40.3%, ARC-AGI-2 5.0%.

## Ideas

* Can better normalization techniques (i.e. remediating super weight) be used to improve training stability?
* Can we plug in a pre-trained LLM as the L-module (frozen or LoRA), while training the H-module on top of it?
* Someone ought to pre-train a frontier-scale HRM.
