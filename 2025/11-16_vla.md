# A Survey on Vision-Language-Action Models: An Action Tokenization Perspective

* Authors: PKU
* Link: https://arxiv.org/abs/2507.01925
* Date: 2025-07-02

## Notes

* LLM/VFM/VLM foundation models confined to the digital world
* VLA models generate actions conditioned on visual and linguistic inputs
* Unified framework: vision and language inputs are iteratively processed through a sequence of VLA modules, producing a
  chain of action tokens that gradually encode increasingly informative and actionable guidance, ultimately producing
  executable actions
* VLA modules are maximal differentiable subnetworks in VLA models that support end-to-end gradient flow, or
  non-differentiable functional units such as motion planning
* 

## Ideas

* 
