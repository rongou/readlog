# A Survey on Vision-Language-Action Models: An Action Tokenization Perspective

* Authors: PKU
* Link: https://arxiv.org/abs/2507.01925
* Date: 2025-07-02

## Notes

* LLM/VFM/VLM foundation models confined to the digital world
* VLA models generate actions conditioned on visual and linguistic inputs
* Unified framework: vision and language inputs are iteratively processed through a sequence of VLA modules, producing a
  chain of action tokens that gradually encode increasingly informative and actionable guidance, ultimately producing
  executable actions
* VLA modules are maximal differentiable subnetworks in VLA models that support end-to-end gradient flow, or
  non-differentiable functional units such as motion planning
* Action tokens: the outputs of VLA modules, and semantically meaningful intermediate representations within VLA modules
* Action token types:
    * language description: A natural language expression that describes the intended action sequence, ranging from
      high-level and abstract language plan to low-level and concrete language motion.
    * code: An executable code snippet or pseudocode that either constitutes a complete robot program or specifies
      low-level atomic operations.
    * affordance: A spatially grounded representation that captures task-specific and interaction-relevant properties of
      objects, typically represented as keypoint, bounding box, segmentation mask, or affordance map.
    * trajectory: A temporally ordered sequence of spatial states that captures the dynamic evolution of an object,
      end-effector, or scene.
    * goal state: A predicted future observation—such as an image, point cloud, or video clip—that visually represents
      the expected outcome of the intended action sequence, serving as an intermediate target for planning and
      execution.
    * latent representation: A purposefully pretrained latent vector sequence that encodes action-relevant information
      over a temporal interval, typically extracted from large-scale datasets.
    * raw action: One or more low-level control commands that can be directly executed by a robot.
    * reasoning: Natural language expressions that explicitly describe the decision-making process leading to a specific
      action token.

## Ideas

* Python all the way?
